---
title: Data 607 Project-Covid Data
author: Alex Moyse
output: 
  html_document:
    df_print: paged
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


<H1>The library load in </H1>

```{r}
library(tidyverse)
library(openxlsx)
library(dplyr)
library(zoo)
```


<H1> The Data Grab </H1>

First things first read the data from the CDC. 

```{r, echo=TRUE}
url.data <- "https://data.cdc.gov/api/views/9mfq-cb36/rows.csv?accessType=DOWNLOAD"
raw <-read.csv(url.data)
raw
```

<H1> Data Engineering </H1>

<H2> Let's Start with Checking the State list </H2>

Firs things first lets grab the unique items from the state column. 

```{r}
sort(unique(raw$state))

```

<H2> Review </H2>

So firstly FSM is a small island Nation, NYC is New York City, which should be combined with New York State. In addition not all states are included in this list, indicating that not all states provided data. Let's see what happens when we look into the data some more. Just going off the back of my hand and a quick skim of the data list, "AS" "NYC" "PR"  "PW"  "RMI" "FSM" "MP" all appear to be not valid options. 

<H1> A small side step </H1>


First things first, I want to create a new data frame linking Created_at and submission date, mostly because I am interested in seeing it, and the transformations to effectively combine and mitigate the data will eliminate deliveries of data. 

```{r}
deltaDate <- subset(raw, select = c(submission_date,created_at) )
deltaDate$date_diff <- as.Date(as.character(deltaDate$submission_date), format="%m/%d/%Y")-
                  as.Date(as.character(deltaDate$created_at), format="%m/%d/%Y %H:%M:%S")
deltaDate <- deltaDate[order(deltaDate$date_diff),]
deltaDate
write.csv(deltaDate,"Covid_data_reporting_delta.csv", row.names = FALSE)
```

<H1> Now let's Address unique states </H1>

Firstly, there are more entries than there are actual states. At just a first glance over, NYC should be incorporated into NY as its a city in a state. IE:

 "AK"  "AL"  "AR"   "AZ"  "CA"  "CO"  "CT"  "DC"  "DE"  "FL" "GA"   "HI"  "IA"  "ID"  "IL"  "IN" "KY"  "LA"  "MA"  "MD"  "ME" "MI"  "MN"  "MO"  "MS"  "MT"  "NC"  "ND"  "NE"  "NH"  "NJ"  "NM"  "NV"  "NY" "OH"  "OK"  "OR"  "PA"  "RI"   "SC"  "SD" "TN"  "TX"  "UT"  "VA"   "VT"  "WA"  "WI"  "WV"  "WY" 

Ans while we are at it, let's see the exact list of what is not a state. 

```{r}
acceptable_states <- c('AK','AL','AR','AZ','CA','CO','CT','DC','DE','FL','GA','HI','IA','ID','IL','IN','KY','KS','LA','MA','MD','ME','MI','MN','MO','MS','MT','NC','ND','NE','NH','NJ','NM','NV','NY','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VA','VT','WA','WI','WV','WY')
all_states <- unique(raw$state)
all_states[!(all_states %in% acceptable_states)]
```

From there, let's merge in the NYC data into the NY data. 


```{r}
data_new <- raw %>%                               # Replacing values
  mutate(state = replace(state, state == 'NYC', 'NY'))
data_new
data_new <- subset(data_new, select = -c(created_at, consent_cases, consent_deaths) )
unique(data_new$state)
data_new


```

And at this point we will proceede to summarise the data, based on the submission date and state. 

```{r}
data_new %>%
  group_by(submission_date, state) %>%
  summarise_all(sum)
```
Now we have the population counts teed up, lets move onto quarters. 

<H1> Let's Define the quarters </H1>

I really didn't like the approach of aggregating by season as it fails to take into account changes in the COVID situation due to time of year, so I opted to add year and quarter in as it standardizes definitions and makes it cleaner.


```{r}
data_new$yearquarter = as.yearqtr(data_new$submission_date, format = "%m/%d/%Y")
data_new2 <- subset(data_new, select = -c(submission_date, state) )
data_new3 <- data_new2 %>%
  group_by(yearquarter) %>%
  summarise_all(sum)
data_new3
write.csv(data_new3,"Covid_data_year_quarter.csv", row.names = FALSE)
```

<H1> Conclusion </H1>

So we have our data saved and easily accessible. It is ready to be read into anything else, or picked up in R to analyze!

